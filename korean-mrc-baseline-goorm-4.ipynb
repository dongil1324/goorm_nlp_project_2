{"cells":[{"cell_type":"markdown","metadata":{"id":"Wb2osD3TRmbw"},"source":["# Korean MRC Baseline\n","\n","## Dependency\n","다음과 같은 라이브러리를 사용한다.\n","- [Konlpy](https://konlpy.org/ko/latest/index.html): 파이썬 한국어 NLP 처리기\n","- [Mecab-korean](https://bitbucket.org/eunjeon/mecab-ko-dic/src): 한국어 형태소 분석기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kfQZtVa0Rmb1"},"outputs":[],"source":["! apt-get install -y openjdk-8-jdk python3-dev\n","! pip install konlpy \"tweepy<4.0.0\"\n","! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13250,"status":"ok","timestamp":1658303058824,"user":{"displayName":"조동일","userId":"13043427476096709949"},"user_tz":-540},"id":"WXgf2EXfjDJR","outputId":"ea9b7af0-1c7e-4289-cb41-c69da7c2df30"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 15.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 68.5 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 14.7 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 61.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}],"source":["! pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"LmSoQlIDRmb2"},"source":["## 데이터셋 구성\n","현재 JSON 데이터를 볼 수 있는 클래스를 하나 작성하자."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"DPf1ccn5Rmb3","executionInfo":{"status":"ok","timestamp":1658671731379,"user_tz":-540,"elapsed":2,"user":{"displayName":"조동일","userId":"13043427476096709949"}}},"outputs":[],"source":["from typing import List, Tuple, Dict, Any\n","import json\n","import random\n","\n","class KoMRC:\n","    def __init__(self, data, indices: List[Tuple[int, int, int]]):\n","        self._data = data\n","        self._indices = indices\n","\n","    # Json을 불러오는 메소드\n","    @classmethod\n","    def load(cls, file_path: str):\n","        with open(file_path, 'r', encoding='utf-8') as fd:\n","            data = json.load(fd)\n","\n","        indices = []\n","        for d_id, document in enumerate(data['data']):\n","            for p_id, paragraph in enumerate(document['paragraphs']):\n","                for q_id, _ in enumerate(paragraph['qas']):\n","                    indices.append((d_id, p_id, q_id))\n","        \n","        return cls(data, indices)\n","\n","    # 데이터 셋을 잘라내는 메소드\n","    @classmethod\n","    def split(cls, dataset, eval_ratio: float=.1, seed=42):\n","        indices = list(dataset._indices)\n","        random.seed(seed)\n","        random.shuffle(indices)\n","        train_indices = indices[int(len(indices) * eval_ratio):]\n","        eval_indices = indices[:int(len(indices) * eval_ratio)]\n","\n","        return cls(dataset._data, train_indices), cls(dataset._data, eval_indices)\n","\n","    def __getitem__(self, index: int) -> Dict[str, Any]:\n","        d_id, p_id, q_id = self._indices[index]\n","        paragraph = self._data['data'][d_id]['paragraphs'][p_id]\n","\n","        context = paragraph['context']\n","        qa = paragraph['qas'][q_id]\n","\n","        guid = qa['guid']\n","        question = qa['question']\n","        answers = qa['answers']\n","\n","        return {\n","            'guid': guid,\n","            'context': context,\n","            'question': question,\n","            'answers': answers\n","        }\n","\n","    def __len__(self) -> int:\n","        return len(self._indices)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20405,"status":"ok","timestamp":1658303079208,"user":{"displayName":"조동일","userId":"13043427476096709949"},"user_tz":-540},"id":"yTFN_3vFhqXE","outputId":"cf559899-317c-452b-e378-fc83f2805a98"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":517,"status":"ok","timestamp":1658303079716,"user":{"displayName":"조동일","userId":"13043427476096709949"},"user_tz":-540},"id":"f2UEQswOhyiq","outputId":"0dc90b33-2dd6-4f4e-bc35-c28f48d3ac01"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/groom/goorm-project-2-korean_mrc/data\n"]}],"source":["%cd '/content/drive/MyDrive/groom/goorm-project-2-korean_mrc/data/'"]},{"cell_type":"markdown","metadata":{"id":"kDnhbvYrRmb4"},"source":["`load` 메소드를 이용해서 Json 데이터를 불러올 수 있다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1966,"status":"ok","timestamp":1658303081679,"user":{"displayName":"조동일","userId":"13043427476096709949"},"user_tz":-540},"id":"m1rqoV-cRmb4","outputId":"6c390eea-583b-4d64-d593-7ad457f9fced"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Samples: 12037\n","{'guid': '798db07f0b9046759deed9d4a35ce31e', 'context': '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.', 'question': '북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?', 'answers': [{'text': '한 달가량', 'answer_start': 478}, {'text': '한 달', 'answer_start': 478}]}\n"]}],"source":["dataset = KoMRC.load('train.json')\n","print(\"Number of Samples:\", len(dataset))\n","print(dataset[0])"]},{"cell_type":"markdown","metadata":{"id":"11doccsDRmb5"},"source":["`split` 메소드를 이용하면 데이터 셋을 나눌 수 있다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658303081679,"user":{"displayName":"조동일","userId":"13043427476096709949"},"user_tz":-540},"id":"JiO3ejHSRmb5","outputId":"a9c6b47a-a533-4348-ccf2-df2418b19d5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Train Samples: 10834\n","Number of Dev Samples: 1203\n","{'guid': '844e22ab28924c1697d5ac28801b34c1', 'context': '지난해 주요 연극상을 나눠 가졌던 세 편의 작품이 올봄에 나란히 앙코르 무대를 갖는다. 대한민국연극대상 연기·무대예술상, 동아연극상 작품·희곡·연기상 등을 수상한 ‘알리바이 연대기’(17~20일 대학로 아르코예술극장 대극장, 25일~5월11일 서계동 국립극단 백성희장민호극장), 연극대상에서 대상과 희곡상을 받은 ‘여기가 집이다’(18일~5월22일 대학로 연우소극장), 연극대상 작품·연출상과 김상열연극상 수상작인 ‘황금용’(5월9~18일 서강대 메리홀 대극장)이다. 초연 당시 짧은 상연 기간과 낮은 인지도 등으로 공연을 놓친 연극팬에겐 평단으로부터 작품성을 인정받은 수작을 관람할 수 있는 기회다. ‘알리바이 연대기’는 희곡을 쓰고 연출한 김재엽의 가족사에 근거한 다큐멘터리 드라마다. 1930년에 태어난 한 개인의 사적인 연대기를 바탕으로 그 사이를 파고드는 역사적 순간들을 정밀하게 조명한다. 연출가는 “공적인 권력이 사적인 권리를 지켜주기보다 억압하기 일쑤였던 한국 현대사 속에서 개인은 언제나 무죄를 입증하며 하루하루 자신을 지켜내야 하는 ‘알리바이의 연대기’ 속에서 살아왔다”고 말한다.한국연극평론가협회는 이 작품을 ‘2013년 올해의 연극 베스트3’로 선정하며 “촘촘하고 세세하게 삶에 천착해 개인과 역사에 대한 이분법적 관점을 극복한다. 정치극에 대한 새로운 가능성을 보여줬다”고 평했다. 이 작품으로 연기상을 휩쓴 남명렬을 비롯해 지춘성 정원조 등 초연 배우들이 그대로 출연한다.‘여기가 집이다’는 허름하고 볼품 없는 ‘20년 전통’의 고시원에 모여 사는 사람들의 절망과 희망을 그린 작품. ‘차력사와 아코디언’ ‘택배 왔어요’를 만든 극단 이와삼의 장우재 대표가 직접 대본을 쓰고 연출했다. 나름의 규칙을 가지고 평화로웠던 고시원에 새로운 주인으로 등장한 ‘20세 고등학생’ 동교가 “이제부터 고시원 식구들에게 월세를 받지 않겠다”고 선언하면서 갑작스런 변화의 바람이 분다.날것 그대로의 직설 화법으로 풀어 놓는 풍성한 인생 이야기와 생동감 넘치는 극적 구조로 ‘집’의 본원적 의미와 삶에 대한 성찰의 기회를 제공한다는 평가를 받았다. 재연에서는 중견 배우 김세동이 장씨 역으로 출연해 박무영 김충근 한동규 류제승 김정민 등 초연 배우들과 호흡을 맞춘다.독일 극작가 롤란트 시멜페니히가 쓴 현대극 ‘황금용’은 독일 소도시에 있는 아시아계 간이식당을 배경으로 현대 물질사회와 세계화 속에 가려진 욕망과 폭력, 소외를 그린다. 치통을 앓지만 불법 체류자 신분으로 치과에 가지 못하는 한 젊은 중국인 요리사는 결국 비참한 최후를 맞는다.작품을 연출한 윤광진 용인대 교수는 “극의 배경은 유럽의 한 소도시이지만 서울이나 경기 안산의 어느 거리에서 일어나는 듯 우리에게 가깝게 다가오는 작품”이라며 “지하철에서 마주치는 외국인 근로자들, 그 옆에서 졸고 있는 우리의 이야기”라고 말했다. 이호성 남미정 이동근 한덕호 방현숙 등 초연 배우들이 다시 뭉친다.', 'question': '윤광진 교수가 연출한 연극이 앙코르 공연을 하는 장소는 어디인가?', 'answers': [{'text': '서강대 메리홀 대극장', 'answer_start': 246}]}\n"]}],"source":["train_dataset, dev_dataset = KoMRC.split(dataset)\n","print(\"Number of Train Samples:\", len(train_dataset))\n","print(\"Number of Dev Samples:\", len(dev_dataset))\n","print(dev_dataset[0])"]},{"cell_type":"markdown","metadata":{"id":"SLAd0jSyRmb6"},"source":["단어 단위로 토큰화해서 정답 위치를 찾기 위하여 토큰화 및 위치 인덱싱을 하는 클래스를 상속을 통해 작성해 보자."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Q-QBhq0Rmb7"},"outputs":[],"source":["from typing import Generator\n","\n","import konlpy\n","\n","class TokenizedKoMRC(KoMRC):\n","    def __init__(self, data, indices: List[Tuple[int, int, int]]) -> None:\n","        super().__init__(data, indices)\n","        self._tagger = konlpy.tag.Mecab()\n","\n","    def _tokenize_with_position(self, sentence: str) -> List[Tuple[str, Tuple[int, int]]]:\n","        position = 0\n","        tokens = []\n","        for morph in self._tagger.morphs(sentence):\n","            position = sentence.find(morph, position)\n","            tokens.append((morph, (position, position + len(morph))))\n","            position += len(morph)\n","        return tokens\n","            \n","    def __getitem__(self, index: int) -> Dict[str, Any]:\n","        sample = super().__getitem__(index)\n","\n","        context, position = zip(*self._tokenize_with_position(sample['context']))\n","        context, position = list(context), list(position)\n","        question = self._tagger.morphs(sample['question'])\n","\n","        if sample['answers'] is not None:\n","            answers = []\n","            for answer in sample['answers']:\n","                for start, (position_start, position_end) in enumerate(position):\n","                    if position_start <= answer['answer_start'] < position_end:\n","                        break\n","                else:\n","                    print(context, answer)\n","                    raise ValueError(\"No mathced start position\")\n","\n","                target = ''.join(answer['text'].split(' '))\n","                source = ''\n","                for end, morph in enumerate(context[start:], start):\n","                    source += morph\n","                    if target in source:\n","                        break\n","                else:\n","                    print(context, answer)\n","                    raise ValueError(\"No Matched end position\")\n","\n","                answers.append({\n","                    'start': start,\n","                    'end': end\n","                })\n","        else:\n","            answers = None\n","        \n","        return {\n","            'guid': sample['guid'],\n","            'context_original': sample['context'],\n","            'context_position': position,\n","            'question_original': sample['question'],\n","            'context': context,\n","            'question': question,\n","            'answers': answers\n","        }\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":417,"status":"ok","timestamp":1658303082093,"user":{"displayName":"조동일","userId":"13043427476096709949"},"user_tz":-540},"id":"2-9y3JsIRmb7","outputId":"1d51628e-dafb-4293-ee09-520a0522dda9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Train Samples: 10834\n","Number of Dev Samples: 1203\n","{'guid': '844e22ab28924c1697d5ac28801b34c1', 'context_original': '지난해 주요 연극상을 나눠 가졌던 세 편의 작품이 올봄에 나란히 앙코르 무대를 갖는다. 대한민국연극대상 연기·무대예술상, 동아연극상 작품·희곡·연기상 등을 수상한 ‘알리바이 연대기’(17~20일 대학로 아르코예술극장 대극장, 25일~5월11일 서계동 국립극단 백성희장민호극장), 연극대상에서 대상과 희곡상을 받은 ‘여기가 집이다’(18일~5월22일 대학로 연우소극장), 연극대상 작품·연출상과 김상열연극상 수상작인 ‘황금용’(5월9~18일 서강대 메리홀 대극장)이다. 초연 당시 짧은 상연 기간과 낮은 인지도 등으로 공연을 놓친 연극팬에겐 평단으로부터 작품성을 인정받은 수작을 관람할 수 있는 기회다. ‘알리바이 연대기’는 희곡을 쓰고 연출한 김재엽의 가족사에 근거한 다큐멘터리 드라마다. 1930년에 태어난 한 개인의 사적인 연대기를 바탕으로 그 사이를 파고드는 역사적 순간들을 정밀하게 조명한다. 연출가는 “공적인 권력이 사적인 권리를 지켜주기보다 억압하기 일쑤였던 한국 현대사 속에서 개인은 언제나 무죄를 입증하며 하루하루 자신을 지켜내야 하는 ‘알리바이의 연대기’ 속에서 살아왔다”고 말한다.한국연극평론가협회는 이 작품을 ‘2013년 올해의 연극 베스트3’로 선정하며 “촘촘하고 세세하게 삶에 천착해 개인과 역사에 대한 이분법적 관점을 극복한다. 정치극에 대한 새로운 가능성을 보여줬다”고 평했다. 이 작품으로 연기상을 휩쓴 남명렬을 비롯해 지춘성 정원조 등 초연 배우들이 그대로 출연한다.‘여기가 집이다’는 허름하고 볼품 없는 ‘20년 전통’의 고시원에 모여 사는 사람들의 절망과 희망을 그린 작품. ‘차력사와 아코디언’ ‘택배 왔어요’를 만든 극단 이와삼의 장우재 대표가 직접 대본을 쓰고 연출했다. 나름의 규칙을 가지고 평화로웠던 고시원에 새로운 주인으로 등장한 ‘20세 고등학생’ 동교가 “이제부터 고시원 식구들에게 월세를 받지 않겠다”고 선언하면서 갑작스런 변화의 바람이 분다.날것 그대로의 직설 화법으로 풀어 놓는 풍성한 인생 이야기와 생동감 넘치는 극적 구조로 ‘집’의 본원적 의미와 삶에 대한 성찰의 기회를 제공한다는 평가를 받았다. 재연에서는 중견 배우 김세동이 장씨 역으로 출연해 박무영 김충근 한동규 류제승 김정민 등 초연 배우들과 호흡을 맞춘다.독일 극작가 롤란트 시멜페니히가 쓴 현대극 ‘황금용’은 독일 소도시에 있는 아시아계 간이식당을 배경으로 현대 물질사회와 세계화 속에 가려진 욕망과 폭력, 소외를 그린다. 치통을 앓지만 불법 체류자 신분으로 치과에 가지 못하는 한 젊은 중국인 요리사는 결국 비참한 최후를 맞는다.작품을 연출한 윤광진 용인대 교수는 “극의 배경은 유럽의 한 소도시이지만 서울이나 경기 안산의 어느 거리에서 일어나는 듯 우리에게 가깝게 다가오는 작품”이라며 “지하철에서 마주치는 외국인 근로자들, 그 옆에서 졸고 있는 우리의 이야기”라고 말했다. 이호성 남미정 이동근 한덕호 방현숙 등 초연 배우들이 다시 뭉친다.', 'context_position': [(0, 3), (4, 6), (7, 9), (9, 10), (10, 11), (12, 14), (15, 17), (17, 18), (19, 20), (21, 22), (22, 23), (24, 26), (26, 27), (28, 29), (29, 30), (30, 31), (32, 35), (36, 39), (40, 42), (42, 43), (44, 45), (45, 47), (47, 48), (49, 53), (53, 55), (55, 57), (58, 60), (60, 61), (61, 63), (63, 65), (65, 66), (66, 67), (68, 70), (70, 72), (72, 73), (74, 76), (76, 77), (77, 79), (79, 80), (80, 82), (82, 83), (84, 85), (85, 86), (87, 89), (89, 90), (91, 92), (92, 96), (97, 100), (100, 102), (102, 104), (104, 105), (105, 107), (107, 108), (109, 111), (111, 112), (113, 116), (116, 118), (118, 120), (121, 124), (124, 125), (126, 128), (128, 129), (129, 130), (130, 131), (131, 132), (132, 134), (134, 135), (136, 139), (140, 144), (145, 148), (148, 151), (151, 153), (153, 154), (154, 155), (156, 158), (158, 160), (160, 162), (163, 165), (165, 166), (167, 168), (168, 170), (170, 171), (172, 173), (173, 174), (175, 176), (176, 178), (178, 179), (180, 181), (181, 182), (182, 183), (183, 185), (185, 187), (187, 188), (188, 189), (189, 190), (190, 191), (191, 193), (193, 194), (195, 197), (197, 198), (199, 201), (201, 204), (204, 205), (205, 206), (207, 209), (209, 211), (212, 214), (214, 215), (215, 217), (217, 218), (218, 219), (220, 223), (223, 225), (225, 226), (227, 229), (229, 231), (232, 233), (233, 236), (236, 238), (238, 239), (239, 240), (240, 241), (241, 242), (242, 244), (244, 245), (246, 249), (250, 252), (252, 253), (254, 257), (257, 258), (258, 259), (259, 260), (260, 261), (262, 264), (265, 267), (268, 269), (269, 270), (271, 273), (274, 276), (276, 277), (278, 279), (279, 280), (281, 284), (285, 286), (286, 288), (289, 291), (291, 292), (293, 295), (296, 298), (298, 299), (299, 301), (302, 304), (304, 308), (309, 311), (311, 312), (312, 313), (314, 317), (317, 318), (319, 321), (321, 322), (323, 325), (325, 326), (327, 328), (329, 330), (330, 331), (332, 334), (334, 335), (335, 336), (337, 338), (338, 342), (343, 346), (346, 347), (347, 348), (349, 351), (351, 352), (353, 354), (354, 355), (356, 358), (358, 359), (360, 363), (363, 364), (365, 368), (368, 369), (370, 372), (372, 373), (374, 379), (380, 383), (383, 384), (384, 385), (386, 390), (390, 391), (391, 392), (393, 396), (397, 398), (399, 401), (401, 402), (403, 404), (404, 405), (405, 406), (407, 410), (410, 411), (412, 414), (414, 416), (417, 418), (419, 421), (421, 422), (423, 426), (426, 427), (428, 430), (430, 431), (432, 434), (434, 435), (435, 436), (437, 439), (439, 440), (440, 441), (442, 444), (444, 446), (446, 447), (448, 451), (451, 452), (453, 454), (454, 455), (455, 456), (456, 457), (458, 460), (460, 461), (462, 463), (463, 464), (464, 465), (466, 468), (468, 469), (470, 472), (472, 473), (473, 474), (474, 476), (477, 479), (479, 480), (480, 481), (482, 484), (484, 485), (485, 486), (487, 489), (490, 493), (494, 495), (495, 497), (498, 500), (500, 501), (502, 505), (506, 508), (508, 509), (510, 512), (512, 513), (513, 514), (515, 519), (520, 522), (522, 523), (524, 526), (526, 527), (527, 528), (529, 530), (530, 531), (532, 533), (533, 537), (537, 538), (539, 542), (542, 543), (544, 545), (545, 547), (548, 551), (551, 552), (552, 553), (553, 554), (555, 556), (556, 558), (558, 559), (559, 561), (561, 563), (563, 566), (566, 568), (568, 569), (570, 571), (572, 574), (574, 575), (576, 577), (577, 581), (581, 582), (583, 585), (585, 586), (587, 589), (590, 593), (593, 594), (594, 595), (595, 596), (597, 599), (599, 600), (600, 601), (602, 603), (603, 605), (605, 606), (606, 607), (608, 610), (610, 611), (611, 612), (613, 614), (614, 615), (616, 618), (618, 619), (620, 622), (622, 623), (624, 626), (626, 627), (628, 630), (631, 634), (634, 635), (636, 638), (638, 639), (640, 642), (642, 644), (644, 645), (646, 648), (648, 649), (649, 650), (651, 653), (654, 657), (658, 660), (660, 661), (661, 662), (663, 665), (665, 666), (666, 667), (667, 668), (668, 669), (670, 672), (672, 673), (673, 674), (675, 676), (677, 679), (679, 681), (682, 684), (684, 685), (685, 686), (687, 689), (690, 693), (693, 694), (695, 697), (697, 698), (699, 702), (703, 706), (707, 708), (709, 711), (712, 714), (714, 715), (715, 716), (717, 720), (721, 723), (723, 725), (725, 726), (726, 727), (727, 729), (729, 730), (731, 732), (732, 733), (733, 734), (734, 735), (735, 736), (737, 739), (739, 740), (740, 741), (742, 744), (745, 746), (746, 747), (748, 749), (749, 751), (751, 752), (753, 755), (755, 756), (756, 757), (758, 761), (761, 762), (763, 765), (766, 767), (767, 768), (769, 771), (771, 772), (772, 773), (774, 776), (776, 777), (778, 780), (780, 781), (782, 784), (785, 787), (787, 788), (789, 790), (790, 791), (791, 793), (793, 794), (795, 799), (799, 800), (801, 802), (802, 804), (805, 806), (806, 808), (808, 809), (809, 810), (811, 813), (814, 816), (817, 818), (818, 819), (819, 820), (820, 821), (822, 825), (826, 828), (828, 829), (830, 832), (833, 835), (835, 836), (837, 838), (838, 839), (840, 842), (842, 843), (843, 844), (844, 845), (846, 848), (848, 849), (850, 852), (852, 853), (854, 856), (856, 857), (858, 860), (860, 862), (862, 863), (864, 867), (867, 868), (869, 872), (873, 875), (875, 877), (878, 880), (880, 881), (882, 883), (883, 885), (885, 886), (887, 891), (891, 892), (893, 895), (895, 896), (897, 898), (898, 900), (900, 902), (903, 906), (907, 909), (909, 910), (910, 912), (913, 915), (915, 916), (917, 918), (918, 919), (920, 921), (921, 922), (922, 923), (923, 924), (924, 925), (926, 928), (928, 929), (929, 931), (932, 936), (937, 939), (939, 940), (941, 943), (943, 944), (945, 947), (947, 948), (948, 949), (949, 950), (951, 954), (954, 955), (956, 958), (959, 961), (961, 963), (964, 965), (965, 966), (967, 968), (968, 969), (970, 972), (972, 973), (974, 976), (977, 980), (980, 981), (982, 985), (986, 988), (988, 989), (990, 991), (991, 992), (993, 995), (995, 996), (997, 998), (998, 999), (999, 1000), (1000, 1001), (1002, 1004), (1004, 1005), (1006, 1008), (1008, 1009), (1010, 1011), (1011, 1012), (1013, 1015), (1016, 1018), (1018, 1019), (1020, 1022), (1022, 1023), (1024, 1026), (1026, 1029), (1030, 1032), (1032, 1033), (1034, 1035), (1035, 1036), (1036, 1037), (1037, 1038), (1039, 1041), (1041, 1043), (1043, 1044), (1045, 1047), (1048, 1050), (1051, 1054), (1054, 1055), (1056, 1057), (1057, 1058), (1059, 1060), (1060, 1062), (1063, 1065), (1065, 1066), (1067, 1070), (1071, 1074), (1075, 1078), (1079, 1082), (1083, 1086), (1087, 1088), (1089, 1091), (1092, 1094), (1094, 1095), (1095, 1096), (1097, 1099), (1099, 1100), (1101, 1104), (1104, 1105), (1105, 1107), (1108, 1111), (1112, 1115), (1116, 1118), (1118, 1121), (1121, 1122), (1123, 1124), (1125, 1128), (1129, 1130), (1130, 1133), (1133, 1134), (1134, 1135), (1136, 1138), (1139, 1140), (1140, 1142), (1142, 1143), (1144, 1145), (1145, 1146), (1147, 1150), (1150, 1151), (1152, 1156), (1156, 1157), (1158, 1160), (1160, 1162), (1163, 1165), (1166, 1168), (1168, 1170), (1170, 1171), (1172, 1174), (1174, 1175), (1176, 1177), (1177, 1178), (1179, 1182), (1183, 1185), (1185, 1186), (1187, 1189), (1189, 1190), (1191, 1193), (1193, 1194), (1195, 1198), (1198, 1199), (1200, 1202), (1202, 1203), (1204, 1205), (1205, 1207), (1208, 1210), (1211, 1213), (1213, 1214), (1215, 1217), (1217, 1219), (1220, 1222), (1222, 1223), (1224, 1225), (1225, 1226), (1227, 1229), (1229, 1230), (1231, 1232), (1233, 1234), (1234, 1235), (1236, 1239), (1240, 1243), (1243, 1244), (1245, 1247), (1248, 1250), (1250, 1251), (1252, 1254), (1254, 1255), (1256, 1257), (1257, 1259), (1259, 1260), (1260, 1262), (1262, 1263), (1264, 1266), (1266, 1267), (1268, 1271), (1272, 1274), (1274, 1275), (1276, 1278), (1278, 1279), (1280, 1281), (1281, 1282), (1282, 1283), (1284, 1286), (1286, 1287), (1288, 1290), (1290, 1291), (1292, 1293), (1294, 1297), (1297, 1298), (1298, 1300), (1301, 1303), (1303, 1305), (1306, 1308), (1309, 1311), (1311, 1312), (1313, 1315), (1316, 1318), (1318, 1320), (1321, 1324), (1324, 1325), (1326, 1327), (1328, 1330), (1330, 1332), (1333, 1335), (1335, 1336), (1337, 1340), (1340, 1341), (1342, 1344), (1344, 1345), (1345, 1346), (1346, 1348), (1349, 1350), (1350, 1353), (1353, 1355), (1356, 1359), (1359, 1360), (1361, 1364), (1365, 1368), (1368, 1369), (1369, 1370), (1371, 1372), (1373, 1374), (1374, 1376), (1377, 1378), (1378, 1379), (1380, 1381), (1381, 1382), (1383, 1385), (1385, 1386), (1387, 1390), (1390, 1391), (1391, 1393), (1394, 1395), (1395, 1396), (1396, 1397), (1397, 1398), (1399, 1402), (1403, 1406), (1407, 1410), (1411, 1414), (1415, 1416), (1416, 1418), (1419, 1420), (1421, 1423), (1424, 1426), (1426, 1427), (1427, 1428), (1429, 1431), (1432, 1435), (1435, 1436)], 'question_original': '윤광진 교수가 연출한 연극이 앙코르 공연을 하는 장소는 어디인가?', 'context': ['지난해', '주요', '연극', '상', '을', '나눠', '가졌', '던', '세', '편', '의', '작품', '이', '올', '봄', '에', '나란히', '앙코르', '무대', '를', '갖', '는다', '.', '대한민국', '연극', '대상', '연기', '·', '무대', '예술', '상', ',', '동아', '연극', '상', '작품', '·', '희곡', '·', '연기', '상', '등', '을', '수상', '한', '‘', '알리바이', '연대기', '’(', '17', '~', '20', '일', '대학', '로', '아르코', '예술', '극장', '대극장', ',', '25', '일', '~', '5', '월', '11', '일', '서계동', '국립극단', '백성희', '장민호', '극장', ')', ',', '연극', '대상', '에서', '대상', '과', '희', '곡상', '을', '받', '은', '‘', '여기', '가', '집', '이', '다', '’(', '18', '일', '~', '5', '월', '22', '일', '대학', '로', '연우', '소극장', ')', ',', '연극', '대상', '작품', '·', '연출', '상', '과', '김상열', '연극', '상', '수상', '작인', '‘', '황금용', '’(', '5', '월', '9', '~', '18', '일', '서강대', '메리', '홀', '대극장', ')', '이', '다', '.', '초연', '당시', '짧', '은', '상연', '기간', '과', '낮', '은', '인지도', '등', '으로', '공연', '을', '놓친', '연극', '팬', '에겐', '평단', '으로부터', '작품', '성', '을', '인정받', '은', '수작', '을', '관람', '할', '수', '있', '는', '기회', '다', '.', '‘', '알리바이', '연대기', '’', '는', '희곡', '을', '쓰', '고', '연출', '한', '김재엽', '의', '가족사', '에', '근거', '한', '다큐멘터리', '드라마', '다', '.', '1930', '년', '에', '태어난', '한', '개인', '의', '사', '적', '인', '연대기', '를', '바탕', '으로', '그', '사이', '를', '파고드', '는', '역사', '적', '순간', '들', '을', '정밀', '하', '게', '조명', '한다', '.', '연출가', '는', '“', '공', '적', '인', '권력', '이', '사', '적', '인', '권리', '를', '지켜', '주', '기', '보다', '억압', '하', '기', '일쑤', '였', '던', '한국', '현대사', '속', '에서', '개인', '은', '언제나', '무죄', '를', '입증', '하', '며', '하루하루', '자신', '을', '지켜', '내', '야', '하', '는', '‘', '알리바이', '의', '연대기', '’', '속', '에서', '살아왔', '다', '”', '고', '말', '한다', '.', '한국', '연극', '평론가', '협회', '는', '이', '작품', '을', '‘', '2013', '년', '올해', '의', '연극', '베스트', '3', '’', '로', '선정', '하', '며', '“', '촘촘', '하', '고', '세세', '하', '게', '삶', '에', '천착', '해', '개인', '과', '역사', '에', '대한', '이분법', '적', '관점', '을', '극복', '한다', '.', '정치', '극', '에', '대한', '새로운', '가능', '성', '을', '보여', '줬', '다', '”', '고', '평했', '다', '.', '이', '작품', '으로', '연기', '상', '을', '휩쓴', '남명렬', '을', '비롯', '해', '지춘성', '정원조', '등', '초연', '배우', '들', '이', '그대로', '출연', '한다', '.', '‘', '여기', '가', '집', '이', '다', '’', '는', '허름', '하', '고', '볼품', '없', '는', '‘', '20', '년', '전통', '’', '의', '고시원', '에', '모여', '사', '는', '사람', '들', '의', '절망', '과', '희망', '을', '그린', '작품', '.', '‘', '차', '력사', '와', '아코디언', '’', '‘', '택배', '왔', '어요', '’', '를', '만든', '극단', '이', '와', '삼', '의', '장우재', '대표', '가', '직접', '대본', '을', '쓰', '고', '연출', '했', '다', '.', '나름', '의', '규칙', '을', '가지', '고', '평화', '로웠', '던', '고시원', '에', '새로운', '주인', '으로', '등장', '한', '‘', '20', '세', '고등학생', '’', '동교', '가', '“', '이제', '부터', '고시원', '식구', '들', '에게', '월세', '를', '받', '지', '않', '겠', '다', '”', '고', '선언', '하', '면서', '갑작스런', '변화', '의', '바람', '이', '분다', '.', '날', '것', '그대로', '의', '직설', '화법', '으로', '풀', '어', '놓', '는', '풍성', '한', '인생', '이야기', '와', '생동감', '넘치', '는', '극', '적', '구조', '로', '‘', '집', '’', '의', '본원', '적', '의미', '와', '삶', '에', '대한', '성찰', '의', '기회', '를', '제공', '한다는', '평가', '를', '받', '았', '다', '.', '재연', '에서', '는', '중견', '배우', '김세동', '이', '장', '씨', '역', '으로', '출연', '해', '박무영', '김충근', '한동규', '류제승', '김정민', '등', '초연', '배우', '들', '과', '호흡', '을', '맞춘다', '.', '독일', '극작가', '롤란트', '시멜', '페니히', '가', '쓴', '현대극', '‘', '황금용', '’', '은', '독일', '소', '도시', '에', '있', '는', '아시아', '계', '간이식당', '을', '배경', '으로', '현대', '물질', '사회', '와', '세계', '화', '속', '에', '가려진', '욕망', '과', '폭력', ',', '소외', '를', '그린다', '.', '치통', '을', '앓', '지만', '불법', '체류', '자', '신분', '으로', '치과', '에', '가', '지', '못하', '는', '한', '젊', '은', '중국인', '요리사', '는', '결국', '비참', '한', '최후', '를', '맞', '는다', '.', '작품', '을', '연출', '한', '윤광진', '용인', '대', '교수', '는', '“', '극', '의', '배경', '은', '유럽', '의', '한', '소도시', '이', '지만', '서울', '이나', '경기', '안산', '의', '어느', '거리', '에서', '일어나', '는', '듯', '우리', '에게', '가깝', '게', '다가오', '는', '작품', '”', '이', '라며', '“', '지하철', '에서', '마주치', '는', '외국인', '근로자', '들', ',', '그', '옆', '에서', '졸', '고', '있', '는', '우리', '의', '이야기', '”', '라고', '말', '했', '다', '.', '이호성', '남미정', '이동근', '한덕호', '방', '현숙', '등', '초연', '배우', '들', '이', '다시', '뭉친다', '.'], 'question': ['윤광진', '교수', '가', '연출', '한', '연극', '이', '앙코르', '공연', '을', '하', '는', '장소', '는', '어디', '인가', '?'], 'answers': [{'start': 125, 'end': 128}]}\n"]}],"source":["dataset = TokenizedKoMRC.load('train.json')\n","\n","train_dataset, dev_dataset = TokenizedKoMRC.split(dataset)\n","print(\"Number of Train Samples:\", len(train_dataset))\n","print(\"Number of Dev Samples:\", len(dev_dataset))\n","print(dev_dataset[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1658303082094,"user":{"displayName":"조동일","userId":"13043427476096709949"},"user_tz":-540},"id":"KUlhxtNhRmb8","outputId":"725a6c0e-96a0-4ce0-f85b-9f1a9ec1bfa5"},"outputs":[{"name":"stdout","output_type":"stream","text":["['서강대', '메리', '홀', '대극장']\n"]}],"source":["sample = dev_dataset[0]\n","print(sample['context'][sample['answers'][0]['start']:sample['answers'][0]['end']+1])"]},{"cell_type":"markdown","metadata":{"id":"jWzWjrJpRmb8"},"source":["## Vocab 생성 및 Indexing\n","토큰화된 데이터 셋을 기준으로 Vocab을 만들고 인덱싱을 하는 `Indexer`를 만들자."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-6fWlqmRmb8"},"outputs":[],"source":["from typing import Sequence\n","from collections import Counter\n","from itertools import chain\n","\n","from tqdm.notebook import tqdm\n","\n","class Indexer:\n","    def __init__(self,\n","        id2token: List[str], \n","        max_length: int=1024,\n","        pad: str='<pad>', unk: str='<unk>', cls: str='<cls>', sep: str='<sep>'\n","    ):\n","        self.pad = pad\n","        self.unk = unk\n","        self.cls = cls\n","        self.sep = sep\n","        self.special_tokens = [pad, unk, cls, sep]\n","\n","        self.max_length = max_length\n","\n","        self.id2token = self.special_tokens + id2token\n","        self.token2id = {token: token_id for token_id, token in enumerate(self.id2token)}\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.id2token)\n","    \n","    @property\n","    def pad_id(self):\n","        return self.token2id[self.pad]\n","    @property\n","    def unk_id(self):\n","        return self.token2id[self.unk]\n","    @property\n","    def cls_id(self):\n","        return self.token2id[self.cls]\n","    @property\n","    def sep_id(self):\n","        return self.token2id[self.sep]\n","\n","    @classmethod\n","    def build_vocab(cls,\n","        dataset: TokenizedKoMRC, \n","        min_freq: int=5\n","    ):\n","        counter = Counter(chain.from_iterable(\n","            sample['context'] + sample['question']\n","            for sample in tqdm(dataset, desc=\"Counting Vocab\")\n","        ))\n","\n","        return cls([word for word, count in counter.items() if count >= min_freq])\n","    \n","    def decode(self,\n","        token_ids: Sequence[int]\n","    ):\n","        return [self.id2token[token_id] for token_id in token_ids]\n","\n","    def sample2ids(self, sample: Dict[str, Any],) -> Dict[str, Any]:\n","        context = [self.token2id.get(token, self.unk_id) for token in sample['context']]\n","        question = [self.token2id.get(token, self.unk_id) for token in sample['question']]\n","\n","        context = context[:self.max_length-len(question)-3]             # Truncate context\n","        \n","        input_ids = [self.cls_id] + question + [self.sep_id] + context + [self.sep_id]\n","        token_type_ids = [0] * (len(question) + 1) + [1] * (len(context) + 2)\n","\n","        if sample['answers'] is not None:\n","            answer = sample['answers'][0]\n","            start = min(answer['start'] + len(question) + 2, self.max_length - 1)\n","            end = min(answer['end'] + len(question) + 2, self.max_length - 1)\n","        else:\n","            start = None\n","            end = None\n","\n","        return {\n","            'guid': sample['guid'],\n","            'context': sample['context_original'],\n","            'question': sample['question_original'],\n","            'position': sample['context_position'],\n","            'input_ids': input_ids,\n","            'token_type_ids': token_type_ids,\n","            'start': start,\n","            'end': end\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87,"referenced_widgets":["5f5f4ef51d744672a1bd3ef1addcc4a4","89ac5f8831f5410a82f4de13ace7592e","59faab93f4d345faa0360d18562e7f35","66aa08d92e974c769ee90b4b8c11204f","3afaf736058a43528b4662ae04abdf1a","a0619cdd077e45c180998980bbeb754b","7a2d3eae71a34b989b37c39a23ab561d","8aa1b4918c8a48be81ed4be33b21a890","aed879ca354c411eb6df9d06fb0afcd0","2112a8d9cb6c43768d5921156de62d46","550e26f18c78413a9563ac57483a2106"]},"executionInfo":{"elapsed":39254,"status":"ok","timestamp":1658303121716,"user":{"displayName":"조동일","userId":"13043427476096709949"},"user_tz":-540},"id":"ySLn5yveRmb9","outputId":"3aeb9a09-d7bc-41b7-e6b1-7dfa66de7aae"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f5f4ef51d744672a1bd3ef1addcc4a4","version_major":2,"version_minor":0},"text/plain":["Counting Vocab:   0%|          | 0/12037 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'guid': '844e22ab28924c1697d5ac28801b34c1', 'context': '지난해 주요 연극상을 나눠 가졌던 세 편의 작품이 올봄에 나란히 앙코르 무대를 갖는다. 대한민국연극대상 연기·무대예술상, 동아연극상 작품·희곡·연기상 등을 수상한 ‘알리바이 연대기’(17~20일 대학로 아르코예술극장 대극장, 25일~5월11일 서계동 국립극단 백성희장민호극장), 연극대상에서 대상과 희곡상을 받은 ‘여기가 집이다’(18일~5월22일 대학로 연우소극장), 연극대상 작품·연출상과 김상열연극상 수상작인 ‘황금용’(5월9~18일 서강대 메리홀 대극장)이다. 초연 당시 짧은 상연 기간과 낮은 인지도 등으로 공연을 놓친 연극팬에겐 평단으로부터 작품성을 인정받은 수작을 관람할 수 있는 기회다. ‘알리바이 연대기’는 희곡을 쓰고 연출한 김재엽의 가족사에 근거한 다큐멘터리 드라마다. 1930년에 태어난 한 개인의 사적인 연대기를 바탕으로 그 사이를 파고드는 역사적 순간들을 정밀하게 조명한다. 연출가는 “공적인 권력이 사적인 권리를 지켜주기보다 억압하기 일쑤였던 한국 현대사 속에서 개인은 언제나 무죄를 입증하며 하루하루 자신을 지켜내야 하는 ‘알리바이의 연대기’ 속에서 살아왔다”고 말한다.한국연극평론가협회는 이 작품을 ‘2013년 올해의 연극 베스트3’로 선정하며 “촘촘하고 세세하게 삶에 천착해 개인과 역사에 대한 이분법적 관점을 극복한다. 정치극에 대한 새로운 가능성을 보여줬다”고 평했다. 이 작품으로 연기상을 휩쓴 남명렬을 비롯해 지춘성 정원조 등 초연 배우들이 그대로 출연한다.‘여기가 집이다’는 허름하고 볼품 없는 ‘20년 전통’의 고시원에 모여 사는 사람들의 절망과 희망을 그린 작품. ‘차력사와 아코디언’ ‘택배 왔어요’를 만든 극단 이와삼의 장우재 대표가 직접 대본을 쓰고 연출했다. 나름의 규칙을 가지고 평화로웠던 고시원에 새로운 주인으로 등장한 ‘20세 고등학생’ 동교가 “이제부터 고시원 식구들에게 월세를 받지 않겠다”고 선언하면서 갑작스런 변화의 바람이 분다.날것 그대로의 직설 화법으로 풀어 놓는 풍성한 인생 이야기와 생동감 넘치는 극적 구조로 ‘집’의 본원적 의미와 삶에 대한 성찰의 기회를 제공한다는 평가를 받았다. 재연에서는 중견 배우 김세동이 장씨 역으로 출연해 박무영 김충근 한동규 류제승 김정민 등 초연 배우들과 호흡을 맞춘다.독일 극작가 롤란트 시멜페니히가 쓴 현대극 ‘황금용’은 독일 소도시에 있는 아시아계 간이식당을 배경으로 현대 물질사회와 세계화 속에 가려진 욕망과 폭력, 소외를 그린다. 치통을 앓지만 불법 체류자 신분으로 치과에 가지 못하는 한 젊은 중국인 요리사는 결국 비참한 최후를 맞는다.작품을 연출한 윤광진 용인대 교수는 “극의 배경은 유럽의 한 소도시이지만 서울이나 경기 안산의 어느 거리에서 일어나는 듯 우리에게 가깝게 다가오는 작품”이라며 “지하철에서 마주치는 외국인 근로자들, 그 옆에서 졸고 있는 우리의 이야기”라고 말했다. 이호성 남미정 이동근 한덕호 방현숙 등 초연 배우들이 다시 뭉친다.', 'question': '윤광진 교수가 연출한 연극이 앙코르 공연을 하는 장소는 어디인가?', 'position': [(0, 3), (4, 6), (7, 9), (9, 10), (10, 11), (12, 14), (15, 17), (17, 18), (19, 20), (21, 22), (22, 23), (24, 26), (26, 27), (28, 29), (29, 30), (30, 31), (32, 35), (36, 39), (40, 42), (42, 43), (44, 45), (45, 47), (47, 48), (49, 53), (53, 55), (55, 57), (58, 60), (60, 61), (61, 63), (63, 65), (65, 66), (66, 67), (68, 70), (70, 72), (72, 73), (74, 76), (76, 77), (77, 79), (79, 80), (80, 82), (82, 83), (84, 85), (85, 86), (87, 89), (89, 90), (91, 92), (92, 96), (97, 100), (100, 102), (102, 104), (104, 105), (105, 107), (107, 108), (109, 111), (111, 112), (113, 116), (116, 118), (118, 120), (121, 124), (124, 125), (126, 128), (128, 129), (129, 130), (130, 131), (131, 132), (132, 134), (134, 135), (136, 139), (140, 144), (145, 148), (148, 151), (151, 153), (153, 154), (154, 155), (156, 158), (158, 160), (160, 162), (163, 165), (165, 166), (167, 168), (168, 170), (170, 171), (172, 173), (173, 174), (175, 176), (176, 178), (178, 179), (180, 181), (181, 182), (182, 183), (183, 185), (185, 187), (187, 188), (188, 189), (189, 190), (190, 191), (191, 193), (193, 194), (195, 197), (197, 198), (199, 201), (201, 204), (204, 205), (205, 206), (207, 209), (209, 211), (212, 214), (214, 215), (215, 217), (217, 218), (218, 219), (220, 223), (223, 225), (225, 226), (227, 229), (229, 231), (232, 233), (233, 236), (236, 238), (238, 239), (239, 240), (240, 241), (241, 242), (242, 244), (244, 245), (246, 249), (250, 252), (252, 253), (254, 257), (257, 258), (258, 259), (259, 260), (260, 261), (262, 264), (265, 267), (268, 269), (269, 270), (271, 273), (274, 276), (276, 277), (278, 279), (279, 280), (281, 284), (285, 286), (286, 288), (289, 291), (291, 292), (293, 295), (296, 298), (298, 299), (299, 301), (302, 304), (304, 308), (309, 311), (311, 312), (312, 313), (314, 317), (317, 318), (319, 321), (321, 322), (323, 325), (325, 326), (327, 328), (329, 330), (330, 331), (332, 334), (334, 335), (335, 336), (337, 338), (338, 342), (343, 346), (346, 347), (347, 348), (349, 351), (351, 352), (353, 354), (354, 355), (356, 358), (358, 359), (360, 363), (363, 364), (365, 368), (368, 369), (370, 372), (372, 373), (374, 379), (380, 383), (383, 384), (384, 385), (386, 390), (390, 391), (391, 392), (393, 396), (397, 398), (399, 401), (401, 402), (403, 404), (404, 405), (405, 406), (407, 410), (410, 411), (412, 414), (414, 416), (417, 418), (419, 421), (421, 422), (423, 426), (426, 427), (428, 430), (430, 431), (432, 434), (434, 435), (435, 436), (437, 439), (439, 440), (440, 441), (442, 444), (444, 446), (446, 447), (448, 451), (451, 452), (453, 454), (454, 455), (455, 456), (456, 457), (458, 460), (460, 461), (462, 463), (463, 464), (464, 465), (466, 468), (468, 469), (470, 472), (472, 473), (473, 474), (474, 476), (477, 479), (479, 480), (480, 481), (482, 484), (484, 485), (485, 486), (487, 489), (490, 493), (494, 495), (495, 497), (498, 500), (500, 501), (502, 505), (506, 508), (508, 509), (510, 512), (512, 513), (513, 514), (515, 519), (520, 522), (522, 523), (524, 526), (526, 527), (527, 528), (529, 530), (530, 531), (532, 533), (533, 537), (537, 538), (539, 542), (542, 543), (544, 545), (545, 547), (548, 551), (551, 552), (552, 553), (553, 554), (555, 556), (556, 558), (558, 559), (559, 561), (561, 563), (563, 566), (566, 568), (568, 569), (570, 571), (572, 574), (574, 575), (576, 577), (577, 581), (581, 582), (583, 585), (585, 586), (587, 589), (590, 593), (593, 594), (594, 595), (595, 596), (597, 599), (599, 600), (600, 601), (602, 603), (603, 605), (605, 606), (606, 607), (608, 610), (610, 611), (611, 612), (613, 614), (614, 615), (616, 618), (618, 619), (620, 622), (622, 623), (624, 626), (626, 627), (628, 630), (631, 634), (634, 635), (636, 638), (638, 639), (640, 642), (642, 644), (644, 645), (646, 648), (648, 649), (649, 650), (651, 653), (654, 657), (658, 660), (660, 661), (661, 662), (663, 665), (665, 666), (666, 667), (667, 668), (668, 669), (670, 672), (672, 673), (673, 674), (675, 676), (677, 679), (679, 681), (682, 684), (684, 685), (685, 686), (687, 689), (690, 693), (693, 694), (695, 697), (697, 698), (699, 702), (703, 706), (707, 708), (709, 711), (712, 714), (714, 715), (715, 716), (717, 720), (721, 723), (723, 725), (725, 726), (726, 727), (727, 729), (729, 730), (731, 732), (732, 733), (733, 734), (734, 735), (735, 736), (737, 739), (739, 740), (740, 741), (742, 744), (745, 746), (746, 747), (748, 749), (749, 751), (751, 752), (753, 755), (755, 756), (756, 757), (758, 761), (761, 762), (763, 765), (766, 767), (767, 768), (769, 771), (771, 772), (772, 773), (774, 776), (776, 777), (778, 780), (780, 781), (782, 784), (785, 787), (787, 788), (789, 790), (790, 791), (791, 793), (793, 794), (795, 799), (799, 800), (801, 802), (802, 804), (805, 806), (806, 808), (808, 809), (809, 810), (811, 813), (814, 816), (817, 818), (818, 819), (819, 820), (820, 821), (822, 825), (826, 828), (828, 829), (830, 832), (833, 835), (835, 836), (837, 838), (838, 839), (840, 842), (842, 843), (843, 844), (844, 845), (846, 848), (848, 849), (850, 852), (852, 853), (854, 856), (856, 857), (858, 860), (860, 862), (862, 863), (864, 867), (867, 868), (869, 872), (873, 875), (875, 877), (878, 880), (880, 881), (882, 883), (883, 885), (885, 886), (887, 891), (891, 892), (893, 895), (895, 896), (897, 898), (898, 900), (900, 902), (903, 906), (907, 909), (909, 910), (910, 912), (913, 915), (915, 916), (917, 918), (918, 919), (920, 921), (921, 922), (922, 923), (923, 924), (924, 925), (926, 928), (928, 929), (929, 931), (932, 936), (937, 939), (939, 940), (941, 943), (943, 944), (945, 947), (947, 948), (948, 949), (949, 950), (951, 954), (954, 955), (956, 958), (959, 961), (961, 963), (964, 965), (965, 966), (967, 968), (968, 969), (970, 972), (972, 973), (974, 976), (977, 980), (980, 981), (982, 985), (986, 988), (988, 989), (990, 991), (991, 992), (993, 995), (995, 996), (997, 998), (998, 999), (999, 1000), (1000, 1001), (1002, 1004), (1004, 1005), (1006, 1008), (1008, 1009), (1010, 1011), (1011, 1012), (1013, 1015), (1016, 1018), (1018, 1019), (1020, 1022), (1022, 1023), (1024, 1026), (1026, 1029), (1030, 1032), (1032, 1033), (1034, 1035), (1035, 1036), (1036, 1037), (1037, 1038), (1039, 1041), (1041, 1043), (1043, 1044), (1045, 1047), (1048, 1050), (1051, 1054), (1054, 1055), (1056, 1057), (1057, 1058), (1059, 1060), (1060, 1062), (1063, 1065), (1065, 1066), (1067, 1070), (1071, 1074), (1075, 1078), (1079, 1082), (1083, 1086), (1087, 1088), (1089, 1091), (1092, 1094), (1094, 1095), (1095, 1096), (1097, 1099), (1099, 1100), (1101, 1104), (1104, 1105), (1105, 1107), (1108, 1111), (1112, 1115), (1116, 1118), (1118, 1121), (1121, 1122), (1123, 1124), (1125, 1128), (1129, 1130), (1130, 1133), (1133, 1134), (1134, 1135), (1136, 1138), (1139, 1140), (1140, 1142), (1142, 1143), (1144, 1145), (1145, 1146), (1147, 1150), (1150, 1151), (1152, 1156), (1156, 1157), (1158, 1160), (1160, 1162), (1163, 1165), (1166, 1168), (1168, 1170), (1170, 1171), (1172, 1174), (1174, 1175), (1176, 1177), (1177, 1178), (1179, 1182), (1183, 1185), (1185, 1186), (1187, 1189), (1189, 1190), (1191, 1193), (1193, 1194), (1195, 1198), (1198, 1199), (1200, 1202), (1202, 1203), (1204, 1205), (1205, 1207), (1208, 1210), (1211, 1213), (1213, 1214), (1215, 1217), (1217, 1219), (1220, 1222), (1222, 1223), (1224, 1225), (1225, 1226), (1227, 1229), (1229, 1230), (1231, 1232), (1233, 1234), (1234, 1235), (1236, 1239), (1240, 1243), (1243, 1244), (1245, 1247), (1248, 1250), (1250, 1251), (1252, 1254), (1254, 1255), (1256, 1257), (1257, 1259), (1259, 1260), (1260, 1262), (1262, 1263), (1264, 1266), (1266, 1267), (1268, 1271), (1272, 1274), (1274, 1275), (1276, 1278), (1278, 1279), (1280, 1281), (1281, 1282), (1282, 1283), (1284, 1286), (1286, 1287), (1288, 1290), (1290, 1291), (1292, 1293), (1294, 1297), (1297, 1298), (1298, 1300), (1301, 1303), (1303, 1305), (1306, 1308), (1309, 1311), (1311, 1312), (1313, 1315), (1316, 1318), (1318, 1320), (1321, 1324), (1324, 1325), (1326, 1327), (1328, 1330), (1330, 1332), (1333, 1335), (1335, 1336), (1337, 1340), (1340, 1341), (1342, 1344), (1344, 1345), (1345, 1346), (1346, 1348), (1349, 1350), (1350, 1353), (1353, 1355), (1356, 1359), (1359, 1360), (1361, 1364), (1365, 1368), (1368, 1369), (1369, 1370), (1371, 1372), (1373, 1374), (1374, 1376), (1377, 1378), (1378, 1379), (1380, 1381), (1381, 1382), (1383, 1385), (1385, 1386), (1387, 1390), (1390, 1391), (1391, 1393), (1394, 1395), (1395, 1396), (1396, 1397), (1397, 1398), (1399, 1402), (1403, 1406), (1407, 1410), (1411, 1414), (1415, 1416), (1416, 1418), (1419, 1420), (1421, 1423), (1424, 1426), (1426, 1427), (1427, 1428), (1429, 1431), (1432, 1435), (1435, 1436)], 'input_ids': [2, 1, 1100, 7, 2226, 71, 4336, 30, 25656, 1139, 97, 57, 39, 2874, 39, 2684, 974, 170, 3, 67, 2912, 4336, 830, 97, 1250, 1278, 553, 1607, 2344, 41, 2239, 30, 4, 3276, 32, 6946, 25656, 4344, 82, 2339, 1386, 15, 1016, 4336, 257, 4355, 695, 4344, 4335, 830, 66, 2412, 4336, 830, 2239, 695, 2386, 695, 4355, 830, 17, 97, 834, 71, 183, 36128, 12542, 588, 8, 64, 90, 9, 276, 142, 1, 4335, 4333, 29910, 66, 132, 9, 64, 246, 130, 530, 9, 24315, 19208, 1, 29106, 4333, 175, 66, 4336, 257, 11, 257, 74, 6503, 1, 97, 649, 20, 183, 2830, 7, 635, 30, 14, 588, 85, 9, 64, 246, 130, 1375, 9, 276, 142, 1, 19205, 175, 66, 4336, 257, 2239, 695, 2226, 830, 74, 33837, 4336, 830, 834, 28079, 183, 1, 588, 246, 130, 1513, 64, 85, 9, 6741, 1807, 1011, 29910, 175, 30, 14, 15, 9493, 1293, 2500, 20, 5450, 135, 74, 981, 20, 2426, 17, 43, 1139, 97, 11653, 4336, 2839, 7081, 9478, 2462, 2239, 332, 97, 4133, 20, 17011, 97, 12211, 407, 402, 38, 39, 950, 14, 15, 183, 36128, 12542, 191, 39, 2386, 97, 2286, 243, 2226, 71, 1, 41, 1, 32, 6784, 71, 13208, 5910, 14, 15, 8803, 126, 32, 1747, 71, 878, 41, 802, 145, 198, 12542, 82, 1802, 43, 248, 476, 82, 16534, 39, 1950, 145, 4419, 264, 97, 8526, 57, 116, 2801, 84, 15, 4373, 39, 425, 3184, 145, 198, 5937, 30, 802, 145, 198, 2172, 82, 11548, 293, 475, 22, 6693, 57, 475, 7874, 482, 553, 149, 6736, 1179, 11, 878, 20, 9520, 11562, 82, 4794, 57, 123, 28915, 554, 97, 11548, 302, 1310, 57, 39, 183, 36128, 41, 12542, 191, 1179, 11, 19596, 14, 443, 243, 26, 84, 15, 149, 4336, 4467, 4684, 39, 30, 2239, 97, 183, 682, 126, 137, 41, 4336, 5487, 65, 191, 142, 196, 57, 123, 425, 19156, 57, 243, 6540, 57, 116, 938, 32, 21429, 93, 878, 74, 1950, 32, 400, 10340, 145, 4724, 97, 10159, 84, 15, 3718, 5739, 32, 400, 1833, 330, 332, 97, 2992, 4367, 14, 443, 243, 9937, 14, 15, 30, 2239, 43, 4355, 830, 97, 9514, 35455, 97, 866, 93, 1, 1, 17, 9493, 4349, 264, 30, 1264, 2238, 84, 15, 183, 2830, 7, 635, 30, 14, 191, 39, 8555, 57, 243, 1, 167, 39, 183, 90, 126, 2884, 191, 41, 28120, 32, 6630, 802, 39, 940, 264, 41, 9099, 74, 2714, 97, 4215, 2239, 15, 183, 423, 8600, 172, 1, 191, 183, 20588, 876, 4369, 191, 82, 1992, 4332, 30, 172, 2390, 41, 1, 1022, 7, 2096, 5951, 97, 2286, 243, 2226, 242, 14, 15, 2349, 41, 4768, 97, 2088, 243, 899, 13201, 553, 28120, 32, 1833, 10353, 43, 1321, 71, 183, 90, 1607, 6412, 191, 1, 7, 425, 1947, 114, 28120, 2892, 264, 806, 3055, 82, 649, 160, 161, 442, 14, 443, 243, 1198, 57, 52, 1743, 345, 41, 4158, 30, 31083, 15, 1706, 99, 1264, 41, 29093, 14646, 43, 1396, 108, 3185, 39, 645, 71, 2291, 2251, 172, 4924, 4943, 39, 5739, 145, 1860, 142, 183, 635, 191, 41, 14031, 145, 1234, 172, 938, 32, 400, 10523, 41, 950, 82, 874, 1327, 244, 82, 649, 464, 14, 15, 25872, 11, 39, 1843, 4349, 1, 30, 1086, 1912, 3113, 43, 2238, 93, 1, 1, 1, 1, 1, 17, 9493, 4349, 264, 74, 16010, 97, 28293, 15, 5172, 9487, 1, 1, 1, 7, 1934, 19200, 183, 1, 191, 20, 5172, 5589, 716, 32, 38, 39, 693, 1038, 1, 97, 5752, 43, 1077, 3418, 325, 172, 712, 189, 1179, 32, 6482, 15736, 74, 3698, 66, 9686, 82, 6957, 15, 1, 97, 10405, 158, 3696, 10469, 1146, 5344, 43, 10702, 32, 7, 160, 4620, 39, 71, 5787, 20, 6679, 8398, 39, 1279, 9568, 71, 14870, 82, 881, 1386, 15, 2239, 97, 2226, 71, 1, 1223, 634, 1100, 39, 425, 5739, 41, 5752, 20, 768, 41, 71, 12050, 30, 158, 16, 794, 151, 6008, 41, 1705, 164, 11, 5575, 39, 2327, 1092, 806, 3114, 116, 5169, 39, 2239, 443, 30, 724, 425, 3126, 11, 21870, 39, 5505, 413, 264, 66, 248, 867, 11, 12581, 243, 38, 39, 1092, 41, 2251, 443, 735, 26, 242, 14, 15, 1, 1, 34839, 1, 2476, 1, 17, 9493, 4349, 264, 30, 91, 25334, 15, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start': 144, 'end': 147}\n"]}],"source":["indexer = Indexer.build_vocab(dataset)\n","print(indexer.sample2ids(dev_dataset[0]))"]},{"cell_type":"markdown","metadata":{"id":"_AEYKS54Rmb9"},"source":["쉽게 Indexer를 활용하기 위해 Indexer가 포함된 데이터 셋을 만들자."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ay7Jj96iRmb9"},"outputs":[],"source":["class IndexerWrappedDataset:\n","    def __init__(self, dataset: TokenizedKoMRC, indexer: Indexer) -> None:\n","        self._dataset = dataset\n","        self._indexer = indexer\n","\n","    def __len__(self) -> int:\n","        return len(self._dataset)\n","    \n","    def __getitem__(self, index: int) -> Dict[str, Any]:\n","        sample = self._indexer.sample2ids(self._dataset[index])\n","        sample['attention_mask'] = [1] * len(sample['input_ids'])\n","\n","        return sample\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":768,"status":"ok","timestamp":1658303121840,"user":{"displayName":"조동일","userId":"13043427476096709949"},"user_tz":-540},"id":"nZ3APNXuRmb-","outputId":"894b5f73-d0c8-4237-bc4e-39b2446796df"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2, 1, 1100, 7, 2226, 71, 4336, 30, 25656, 1139, 97, 57, 39, 2874, 39, 2684, 974, 170, 3, 67, 2912, 4336, 830, 97, 1250, 1278, 553, 1607, 2344, 41, 2239, 30, 4, 3276, 32, 6946, 25656, 4344, 82, 2339, 1386, 15, 1016, 4336, 257, 4355, 695, 4344, 4335, 830, 66, 2412, 4336, 830, 2239, 695, 2386, 695, 4355, 830, 17, 97, 834, 71, 183, 36128, 12542, 588, 8, 64, 90, 9, 276, 142, 1, 4335, 4333, 29910, 66, 132, 9, 64, 246, 130, 530, 9, 24315, 19208, 1, 29106, 4333, 175, 66, 4336, 257, 11, 257, 74, 6503, 1, 97, 649, 20, 183, 2830, 7, 635, 30, 14, 588, 85, 9, 64, 246, 130, 1375, 9, 276, 142, 1, 19205, 175, 66, 4336, 257, 2239, 695, 2226, 830, 74, 33837, 4336, 830, 834, 28079, 183, 1, 588, 246, 130, 1513, 64, 85, 9, 6741, 1807, 1011, 29910, 175, 30, 14, 15, 9493, 1293, 2500, 20, 5450, 135, 74, 981, 20, 2426, 17, 43, 1139, 97, 11653, 4336, 2839, 7081, 9478, 2462, 2239, 332, 97, 4133, 20, 17011, 97, 12211, 407, 402, 38, 39, 950, 14, 15, 183, 36128, 12542, 191, 39, 2386, 97, 2286, 243, 2226, 71, 1, 41, 1, 32, 6784, 71, 13208, 5910, 14, 15, 8803, 126, 32, 1747, 71, 878, 41, 802, 145, 198, 12542, 82, 1802, 43, 248, 476, 82, 16534, 39, 1950, 145, 4419, 264, 97, 8526, 57, 116, 2801, 84, 15, 4373, 39, 425, 3184, 145, 198, 5937, 30, 802, 145, 198, 2172, 82, 11548, 293, 475, 22, 6693, 57, 475, 7874, 482, 553, 149, 6736, 1179, 11, 878, 20, 9520, 11562, 82, 4794, 57, 123, 28915, 554, 97, 11548, 302, 1310, 57, 39, 183, 36128, 41, 12542, 191, 1179, 11, 19596, 14, 443, 243, 26, 84, 15, 149, 4336, 4467, 4684, 39, 30, 2239, 97, 183, 682, 126, 137, 41, 4336, 5487, 65, 191, 142, 196, 57, 123, 425, 19156, 57, 243, 6540, 57, 116, 938, 32, 21429, 93, 878, 74, 1950, 32, 400, 10340, 145, 4724, 97, 10159, 84, 15, 3718, 5739, 32, 400, 1833, 330, 332, 97, 2992, 4367, 14, 443, 243, 9937, 14, 15, 30, 2239, 43, 4355, 830, 97, 9514, 35455, 97, 866, 93, 1, 1, 17, 9493, 4349, 264, 30, 1264, 2238, 84, 15, 183, 2830, 7, 635, 30, 14, 191, 39, 8555, 57, 243, 1, 167, 39, 183, 90, 126, 2884, 191, 41, 28120, 32, 6630, 802, 39, 940, 264, 41, 9099, 74, 2714, 97, 4215, 2239, 15, 183, 423, 8600, 172, 1, 191, 183, 20588, 876, 4369, 191, 82, 1992, 4332, 30, 172, 2390, 41, 1, 1022, 7, 2096, 5951, 97, 2286, 243, 2226, 242, 14, 15, 2349, 41, 4768, 97, 2088, 243, 899, 13201, 553, 28120, 32, 1833, 10353, 43, 1321, 71, 183, 90, 1607, 6412, 191, 1, 7, 425, 1947, 114, 28120, 2892, 264, 806, 3055, 82, 649, 160, 161, 442, 14, 443, 243, 1198, 57, 52, 1743, 345, 41, 4158, 30, 31083, 15, 1706, 99, 1264, 41, 29093, 14646, 43, 1396, 108, 3185, 39, 645, 71, 2291, 2251, 172, 4924, 4943, 39, 5739, 145, 1860, 142, 183, 635, 191, 41, 14031, 145, 1234, 172, 938, 32, 400, 10523, 41, 950, 82, 874, 1327, 244, 82, 649, 464, 14, 15, 25872, 11, 39, 1843, 4349, 1, 30, 1086, 1912, 3113, 43, 2238, 93, 1, 1, 1, 1, 1, 17, 9493, 4349, 264, 74, 16010, 97, 28293, 15, 5172, 9487, 1, 1, 1, 7, 1934, 19200, 183, 1, 191, 20, 5172, 5589, 716, 32, 38, 39, 693, 1038, 1, 97, 5752, 43, 1077, 3418, 325, 172, 712, 189, 1179, 32, 6482, 15736, 74, 3698, 66, 9686, 82, 6957, 15, 1, 97, 10405, 158, 3696, 10469, 1146, 5344, 43, 10702, 32, 7, 160, 4620, 39, 71, 5787, 20, 6679, 8398, 39, 1279, 9568, 71, 14870, 82, 881, 1386, 15, 2239, 97, 2226, 71, 1, 1223, 634, 1100, 39, 425, 5739, 41, 5752, 20, 768, 41, 71, 12050, 30, 158, 16, 794, 151, 6008, 41, 1705, 164, 11, 5575, 39, 2327, 1092, 806, 3114, 116, 5169, 39, 2239, 443, 30, 724, 425, 3126, 11, 21870, 39, 5505, 413, 264, 66, 248, 867, 11, 12581, 243, 38, 39, 1092, 41, 2251, 443, 735, 26, 242, 14, 15, 1, 1, 34839, 1, 2476, 1, 17, 9493, 4349, 264, 30, 91, 25334, 15, 3] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 144 147\n"]}],"source":["indexed_train_dataset = IndexerWrappedDataset(train_dataset, indexer)\n","indexed_dev_dataset = IndexerWrappedDataset(dev_dataset, indexer)\n","\n","sample = indexed_dev_dataset[0]\n","print(sample['input_ids'], sample['attention_mask'], sample['token_type_ids'], sample['start'], sample['end'])"]},{"cell_type":"markdown","metadata":{"id":"EQYdl_82Rmb-"},"source":["## Transformer Encoder를 활용한 MRC 모델\n","![Bert for MRC](https://miro.medium.com/max/340/1*cXDOP0gsE7Zp8-sgZqYfTA.png)\n","\n","Transformer 인코더 마지막에 Linear Layer를 붙여 정답의 시작과 끝을 맞추는 간단한 모델을 생성보자."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZuZNlr_Rmb-"},"outputs":[],"source":["import torch.nn as nn\n","\n","from transformers.models.bert.modeling_bert import (\n","    BertModel,\n","    BertPreTrainedModel\n",")\n","\n","## Simple Version for Bert QA: https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForQuestionAnswering.forward\n","class BertForQuestionAnswering(BertPreTrainedModel):\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.bert = BertModel(config, add_pooling_layer=False)\n","        self.start_linear = nn.Linear(config.hidden_size, 1)\n","        self.end_linear = nn.Linear(config.hidden_size, 1)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None, #Baseline 코드의 경우 question 까지 포함된 input_ids 에 대해 linear projection 진행.\n","        attention_mask=None,\n","        token_type_ids=None\n","    ):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","        )\n","\n","        start_logits = self.start_linear(outputs.last_hidden_state).squeeze(-1)\n","        end_logits = self.end_linear(outputs.last_hidden_state).squeeze(-1)\n","\n","        return start_logits, end_logits"]},{"cell_type":"markdown","metadata":{"id":"GIMhCDaGRmb-"},"source":["## 학습 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tI8MaSm-Rmb_"},"outputs":[],"source":["import torch\n","from torch.nn.utils.rnn import pad_sequence\n","\n","class Collator:\n","    def __init__(self, indexer: Indexer) -> None:\n","        self._indexer = indexer\n","\n","    def __call__(self, samples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n","        samples = {key: [sample[key] for sample in samples] for key in samples[0] }\n","\n","        for key in 'start', 'end':\n","            if samples[key][0] is None:\n","                samples[key] = None\n","            else:\n","                samples[key] = torch.tensor(samples[key], dtype=torch.long)\n","        for key in 'input_ids', 'attention_mask', 'token_type_ids':\n","            samples[key] = pad_sequence([torch.tensor(sample, dtype=torch.long) for sample in samples[key]],\n","                                        batch_first=True, \n","                                        padding_value=self._indexer.pad_id)\n","        return samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDoALdR9Rmb_"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","batch_size = 128\n","accumulation = 4 # 메모리를 아끼기 위하여 Gradient accumulation을 해보자\n","\n","collator = Collator(indexer)\n","train_loader = DataLoader(indexed_train_dataset, batch_size=batch_size//accumulation, shuffle=True, collate_fn=collator, num_workers=2)\n","dev_loader = DataLoader(indexed_dev_dataset, batch_size=batch_size//accumulation, shuffle=False, collate_fn=collator, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":619,"status":"ok","timestamp":1658303123492,"user":{"displayName":"조동일","userId":"13043427476096709949"},"user_tz":-540},"id":"hk4eYPcWRmb_","outputId":"972210b3-db04-4cf2-f37c-560372644442"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 1018])\n","tensor([[    2,     1,  1100,  ...,     0,     0,     0],\n","        [    2,   317,  3922,  ...,     0,     0,     0],\n","        [    2,    71,  6098,  ...,     0,     0,     0],\n","        ...,\n","        [    2,  6721,  2033,  ...,     0,     0,     0],\n","        [    2,  2469,   172,  ...,     0,     0,     0],\n","        [    2,  2470, 19480,  ...,     0,     0,     0]])\n","['guid', 'context', 'question', 'position', 'input_ids', 'token_type_ids', 'start', 'end', 'attention_mask']\n"]}],"source":["batch = next(iter(dev_loader))\n","print(batch['input_ids'].shape)\n","print(batch['input_ids'])\n","print(list(batch.keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHS7Tc0RRmcA"},"outputs":[],"source":["import torch\n","from transformers import BertConfig\n","\n","torch.manual_seed(42)\n","config = BertConfig(\n","     vocab_size=indexer.vocab_size,\n","     max_position_embeddings=1024,\n","     hidden_size=256,\n","     num_hidden_layers=4,\n","     num_attention_heads=4,\n","     intermediate_size=1024\n",")\n","#model = BertForQuestionAnswering(config)\n","# model.cuda()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["2091503cc602407fb6f714460ae0480b","d186885eec6e4b4c82d47bd6e59d9597","51299184249347eeb4175a2f8440e11d"]},"id":"mQ4dz0Z_RmcA","outputId":"bfb2fdd1-b3d4-404b-8b34-22617dd4354c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2091503cc602407fb6f714460ae0480b","version_major":2,"version_minor":0},"text/plain":["Train:   0%|          | 0/678 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["train score: 10.483\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d186885eec6e4b4c82d47bd6e59d9597","version_major":2,"version_minor":0},"text/plain":["Evaluation:   0%|          | 0/76 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Evaluation score: 9.919\n","Epoch 2\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51299184249347eeb4175a2f8440e11d","version_major":2,"version_minor":0},"text/plain":["Train:   0%|          | 0/678 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import os\n","from statistics import mean\n","\n","import torch.nn.functional as F\n","from torch.nn.utils import clip_grad_norm_\n","\n","os.makedirs('dump', exist_ok=True)\n","train_losses = []\n","dev_losses = []\n","\n","step = 0\n","\n","for epoch in range(1, 31):\n","    print(\"Epoch\", epoch)\n","    # Training\n","    running_loss = 0.\n","    losses = []\n","    progress_bar = tqdm(train_loader, desc='Train')\n","    for batch in progress_bar:\n","        del batch['guid'], batch['context'], batch['question'], batch['position']\n","        # batch = {key: value.cuda() for key, value in batch.items()}\n","        start = batch.pop('start')\n","        end = batch.pop('end')\n","        \n","        start_logits, end_logits = model(**batch)\n","        \n","        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n","        (loss / accumulation).backward()\n","        running_loss += loss.item()\n","        del batch, start, end, start_logits, end_logits, loss\n","        \n","        step += 1\n","        if step % accumulation:\n","            continue\n","\n","        clip_grad_norm_(model.parameters(), max_norm=1.)\n","        optimizer.step()\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        losses.append(running_loss / accumulation)\n","        running_loss = 0.\n","        progress_bar.set_description(f\"Train - Loss: {losses[-1]:.3f}\")\n","    train_losses.append(mean(losses))\n","    print(f\"train score: {train_losses[-1]:.3f}\")\n","\n","    # Evaluation\n","    losses = []\n","    for batch in tqdm(dev_loader, desc=\"Evaluation\"):\n","        del batch['guid'], batch['context'], batch['question'], batch['position']\n","        # batch = {key: value.cuda() for key, value in batch.items()}\n","        start = batch.pop('start')\n","        end = batch.pop('end')\n","        \n","        with torch.no_grad():\n","            start_logits, end_logits = model(**batch)\n","        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n","\n","        losses.append(loss.item())\n","        del batch, start, end, start_logits, end_logits, loss\n","    dev_losses.append(mean(losses))\n","    print(f\"Evaluation score: {dev_losses[-1]:.3f}\")\n","\n","    model.save_pretrained(f'dump/model.{epoch}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6pEshNJRmcA"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","t = list(range(1, 31))\n","plt.plot(t, train_losses, label=\"Train Loss\")\n","plt.plot(t, dev_losses, label=\"Dev Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"IJ8FRIhhRmcA"},"source":["![loss_plot](https://github.com/mynsng/mynsng.github.io/blob/master/assets/images/__results___26_0.png?raw=true)"]},{"cell_type":"markdown","metadata":{"id":"khPV7ViyRmcA"},"source":["학습 데이터 셋에 Overfitting이 일어나는 것을 확인할 수 있다."]},{"cell_type":"markdown","metadata":{"id":"QqzcfiJmRmcB"},"source":["## Answer Inference\n","모델의 Output을 활용해서 질문의 답을 찾는 코드를 작성하자."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sp5k5RqgRmcB"},"outputs":[],"source":["model = BertForQuestionAnswering.from_pretrained('dump/model.30')\n","model.cuda()\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zu10G0zWRmcB"},"outputs":[],"source":["for idx, sample in zip(range(1, 4), indexed_train_dataset):\n","    print(f'------{idx}------')\n","    print('Context:', sample['context'])\n","    print('Question:', sample['question'])\n","    \n","    input_ids, token_type_ids = [\n","        torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n","        for key in (\"input_ids\", \"token_type_ids\")\n","    ]\n","    \n","    with torch.no_grad():\n","        start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n","    start_logits.squeeze_(0), end_logits.squeeze_(0)\n","    \n","    start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","    end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","    probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n","    index = torch.argmax(probability).item()\n","    \n","    start = index // len(end_prob)\n","    end = index % len(end_prob)\n","    \n","    start = sample['position'][start][0]\n","    end = sample['position'][end][1]\n","\n","    print('Answer:', sample['context'][start:end])"]},{"cell_type":"markdown","metadata":{"id":"9Spa_5yLRmcB"},"source":["## Test 출력 파일 작성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-vulvCsRmcB"},"outputs":[],"source":["test_dataset = TokenizedKoMRC.load('test.json')\n","test_dataset = IndexerWrappedDataset(test_dataset, indexer)\n","print(\"Number of Test Samples\", len(test_dataset))\n","print(test_dataset[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"roD7y1ZwRmcB"},"outputs":[],"source":["import csv\n","\n","os.makedirs('out', exist_ok=True)\n","with torch.no_grad(), open('out/baseline.csv', 'w') as fd:\n","    writer = csv.writer(fd)\n","    writer.writerow(['Id', 'Predicted'])\n","\n","    rows = []\n","    for sample in tqdm(test_dataset, \"Testing\"):\n","        input_ids, token_type_ids = [\n","            torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n","            for key in (\"input_ids\", \"token_type_ids\")\n","        ]\n","    \n","        with torch.no_grad():\n","            start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n","        start_logits.squeeze_(0), end_logits.squeeze_(0)\n","    \n","        start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","        end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n","        probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n","        index = torch.argmax(probability).item()\n","    \n","        start = index // len(end_prob)\n","        end = index % len(end_prob)\n","    \n","        start = sample['position'][start][0]\n","        end = sample['position'][end][1]\n","\n","        rows.append([sample[\"guid\"], sample['context'][start:end]])\n","    \n","    writer.writerows(rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jtobdarQRmcB"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y7SS9K8DRmcC"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"name":"korean-mrc-baseline-goorm-4.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2112a8d9cb6c43768d5921156de62d46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3afaf736058a43528b4662ae04abdf1a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"550e26f18c78413a9563ac57483a2106":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59faab93f4d345faa0360d18562e7f35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8aa1b4918c8a48be81ed4be33b21a890","max":12037,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aed879ca354c411eb6df9d06fb0afcd0","value":12037}},"5f5f4ef51d744672a1bd3ef1addcc4a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89ac5f8831f5410a82f4de13ace7592e","IPY_MODEL_59faab93f4d345faa0360d18562e7f35","IPY_MODEL_66aa08d92e974c769ee90b4b8c11204f"],"layout":"IPY_MODEL_3afaf736058a43528b4662ae04abdf1a"}},"66aa08d92e974c769ee90b4b8c11204f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2112a8d9cb6c43768d5921156de62d46","placeholder":"​","style":"IPY_MODEL_550e26f18c78413a9563ac57483a2106","value":" 12037/12037 [00:38&lt;00:00, 389.53it/s]"}},"7a2d3eae71a34b989b37c39a23ab561d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89ac5f8831f5410a82f4de13ace7592e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0619cdd077e45c180998980bbeb754b","placeholder":"​","style":"IPY_MODEL_7a2d3eae71a34b989b37c39a23ab561d","value":"Counting Vocab: 100%"}},"8aa1b4918c8a48be81ed4be33b21a890":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0619cdd077e45c180998980bbeb754b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aed879ca354c411eb6df9d06fb0afcd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}